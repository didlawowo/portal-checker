import asyncio
import os
import ssl
import sys
import time
from typing import Dict, List, Union

import aiohttp
import certifi
import tomllib
import yaml
from flask import (
    Flask,
    jsonify,
    redirect,
    render_template,
    request,
    send_from_directory,
)
from icecream import ic
from kubernetes import client, config
from loguru import logger


def serialize_record(record):
    """
    Personnalise la s√©rialisation des records pour le format JSON.
    Cette fonction est appel√©e pour chaque enregistrement de log quand serialize=True.
    Elle transforme les attributs du record en un format JSON compatible.
    """
    subset = {
        "timestamp": record["time"].strftime("%Y-%m-%d %H:%M:%S.%f"),
        "level": record["level"].name,
        "message": record["message"],
        "module": record["module"],
        "function": record["function"],
        "line": record["line"],
        "process": {"id": record["process"].id, "name": record["process"].name},
        "thread": {"id": record["thread"].id, "name": record["thread"].name},
    }

    # Ajout des informations suppl√©mentaires si elles existent
    if record["extra"]:
        subset["extra"] = record["extra"]

    if record["exception"] is not None:
        subset["exception"] = record["exception"]

    return subset


def get_app_version() -> str:
    """Get application version from pyproject.toml"""
    # Try multiple possible locations
    possible_paths = [
        "pyproject.toml",
        "/app/pyproject.toml",
        os.path.join(os.path.dirname(__file__), "pyproject.toml"),
    ]

    for path in possible_paths:
        try:
            with open(path, "rb") as f:
                data = tomllib.load(f)
                return data.get("project", {}).get("version", "unknown")
        except (FileNotFoundError, tomllib.TOMLDecodeError):
            continue

    return "unknown"


def setup_logger(log_format: str = "text", log_level: str = "INFO") -> None:
    """
    Configure le logger selon le format souhait√© (texte ou JSON).

    Le mode texte est optimis√© pour la lisibilit√© humaine avec des couleurs,
    tandis que le mode JSON est con√ßu pour √™tre pars√© par des outils d'analyse de logs.
    """
    # Supprime les handlers existants pour √©viter la duplication
    logger.remove()

    if log_format.lower() == "json":
        # Configuration pour le format JSON
        logger.add(
            sys.stdout,
            level=log_level,
            serialize=True,  # Active la s√©rialisation JSON
            format="{message}",  # Format minimal car g√©r√© par serialize_record
            enqueue=True,  # Rend le logging thread-safe
            backtrace=True,  # Inclut les stack traces d√©taill√©es
            diagnose=False,  # D√©sactive l'affichage des variables locales dans les traces
            catch=True,  # Capture les erreurs de logging
        )
    else:
        # Configuration pour le format texte lisible
        logger.add(
            sys.stdout,
            format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | <level>{message}</level>",
            level=log_level,
            colorize=True,
            enqueue=True,
            backtrace=True,
            diagnose=True,
            catch=True,
        )


# Configuration initiale du logger
LOG_FORMAT = os.getenv("LOG_FORMAT", "json")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
setup_logger(LOG_FORMAT, LOG_LEVEL)


# Configuration
SLACK_NOTIFICATIONS_ENABLED = (
    os.getenv("ENABLE_SLACK_NOTIFICATIONS", "false").lower() == "true"
)
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL", "")
TIMEOUT = int(os.getenv("REQUEST_TIMEOUT", "5"))  # ‚öôÔ∏è Timeout configurable
MAX_CONCURRENT_REQUESTS = int(
    os.getenv("MAX_CONCURRENT_REQUESTS", "10")
)  # üîÑ Contr√¥le de la concurrence

URLS_FILE = os.getenv(
    "URLS_FILE",
    "config/urls.yaml"
    if os.getenv("FLASK_ENV") == "development"
    else "/app/data/urls.yaml",
)
EXCLUDED_URLS_FILE = os.getenv(
    "EXCLUDED_URLS_FILE",
    "config/excluded-urls.yaml"
    if os.getenv("FLASK_ENV") == "development"
    else "/app/config/excluded-urls.yaml",
)

# üöÄ Auto-refresh activ√© par d√©faut
AUTO_REFRESH_ON_START = os.getenv("AUTO_REFRESH_ON_START", "true").lower() == "true"


def sslMode():
    if os.getenv("CUSTOM_CERT"):
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        custom_cert_path = os.getenv("CUSTOM_CERT", "certs/")
        ic(custom_cert_path)
        if os.path.exists(custom_cert_path):
            try:
                ssl_context.load_verify_locations(capath=custom_cert_path)
                logger.info(
                    f"‚úÖ Certificat personnalis√© charg√©: {len(ssl_context.get_ca_certs())}"
                )
            except Exception as e:
                logger.warning(
                    f"‚ö†Ô∏è Impossible de charger le certificat personnalis√©: {str(e)}"
                )
        return ssl_context
    else:
        logger.info("‚ö†Ô∏è  V√©rification SSL d√©sactiv√©e")
        return False


def load_excluded_urls():
    """
    üîß Charge les URLs exclues avec gestion robuste des formats
    Supporte: liste YAML, fichier vide, format incorrect
    """
    excluded_urls = set()

    if not os.path.exists(EXCLUDED_URLS_FILE):
        logger.info(f"‚ÑπÔ∏è Aucun fichier d'exclusion trouv√© √† {EXCLUDED_URLS_FILE}")
        return excluded_urls

    try:
        with open(EXCLUDED_URLS_FILE, "r", encoding="utf-8") as file:
            content = file.read().strip()

            # üîç Fichier vide ou contenant seulement des espaces
            if not content:
                logger.info(
                    "‚ÑπÔ∏è Fichier d'exclusions vide, toutes les URLs seront test√©es"
                )
                return excluded_urls

            # üìù Chargement du YAML
            data = yaml.safe_load(content)

            # ‚úÖ V√©rification du format attendu (liste)
            if data is None:
                logger.info(
                    "‚ÑπÔ∏è Fichier d'exclusions vide (null), toutes les URLs seront test√©es"
                )
                return excluded_urls
            elif isinstance(data, list):
                excluded_urls = set(data)
                logger.info(
                    f"‚úÖ {len(excluded_urls)} URLs exclues charg√©es depuis {EXCLUDED_URLS_FILE}"
                )
                for url in excluded_urls:
                    logger.debug(f"üö´ URL exclue: {url}")
                return excluded_urls
            else:
                logger.error(
                    f"‚ùå Format YAML incorrect dans {EXCLUDED_URLS_FILE} - Attendu: liste, Re√ßu: {type(data)}"
                )
                logger.error(f"‚ùå Contenu: {data}")
                return excluded_urls

    except yaml.YAMLError as e:
        logger.error(f"‚ùå Erreur YAML lors du chargement des exclusions: {str(e)}")
        return excluded_urls
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement des URLs exclues: {str(e)}")
        return excluded_urls


app = Flask(__name__)

excluded_urls = load_excluded_urls()
ssl_context = sslMode()


async def send_slack_alert_async(
    session: aiohttp.ClientSession,
    url: str,
    status_code: Union[int, str],
    details: str = "",
) -> None:
    """Version asynchrone de l'alerte Slack"""
    if not SLACK_NOTIFICATIONS_ENABLED or not SLACK_WEBHOOK_URL:
        return

    try:
        if isinstance(status_code, str):
            try:
                status_code = int(status_code)
            except ValueError:
                status_code = 500

        if 200 <= status_code < 300 or 400 <= status_code < 500:
            return

        message = {
            "blocks": [
                {
                    "type": "header",
                    "text": {
                        "type": "plain_text",
                        "text": "üö® Alerte Ingress",
                        "emoji": True,
                    },
                },
                {
                    "type": "section",
                    "fields": [
                        {"type": "mrkdwn", "text": f"*URL:*\n{url}"},
                        {"type": "mrkdwn", "text": f"*Status:*\n{status_code}"},
                    ],
                },
            ]
        }

        if details:
            message["blocks"].append(
                {
                    "type": "section",
                    "text": {"type": "mrkdwn", "text": f"*D√©tails:*\n{details}"},
                }
            )

        async with session.post(
            SLACK_WEBHOOK_URL,
            json=message,
            timeout=aiohttp.ClientTimeout(total=TIMEOUT),
        ) as response:
            await response.text()

    except Exception as e:
        logger.error(f"Erreur lors de l'envoi de l'alerte Slack: {str(e)}")


async def check_single_url(session: aiohttp.ClientSession, data: dict) -> Dict:
    """Test une seule URL de mani√®re asynchrone

    Args:
        session: Session aiohttp pour r√©utiliser les connexions
        data: Dictionnaire contenant les donn√©es de l'URL √† tester
    Returns:
        Dict avec le statut du test
    """
    url = data.get("url", "")  # R√©cup√©rer l'URL de mani√®re s√©curis√©e
    start_time = time.time()

    try:
        logger.debug(f"Test de l'URL {url}")
        full_url = f"https://{url}" if not url.startswith("http") else url

        async with session.get(
            full_url, timeout=aiohttp.ClientTimeout(total=TIMEOUT), ssl=ssl_context
        ) as response:
            response_time = round(
                (time.time() - start_time) * 1000
            )  # ms, arrondi √† l'unit√©
            status_code = response.status

            details = ""
            if status_code != 200 and status_code != 401:
                details = f"‚ùå {response.reason}"
                logger.error(f"Erreur pour l'URL {full_url}")
                if SLACK_NOTIFICATIONS_ENABLED:
                    await send_slack_alert_async(session, url, status_code, details)

            if status_code == 404:
                details = "‚ùì Not Found"
                # Envoyer une alerte Slack
                if SLACK_NOTIFICATIONS_ENABLED:
                    await send_slack_alert_async(session, url, status_code, details)

            # Mettre √† jour le dictionnaire original avec les r√©sultats
            data["status"] = status_code
            data["details"] = details
            data["response_time"] = response_time

            logger.debug(
                f"Test de l'URL {url} : {status_code}, {response_time}ms, data: {data}"
            )

            return data

    except asyncio.TimeoutError:
        response_time = round((time.time() - start_time) * 1000)
        result = data.copy()
        result.update(
            {
                "status": 504,  # üïí Gateway Timeout plus appropri√© que 500
                "details": "‚ùå Timeout Error",
                "response_time": response_time,
            }
        )
        return result

    except Exception as e:
        response_time = round((time.time() - start_time) * 1000)
        result = data.copy()
        result.update(
            {
                "status": 500,
                "details": f"‚ùå Error: {str(e)}",
                "response_time": response_time,
            }
        )
        return result


async def check_urls_async(file_path: str | None = None) -> List[Dict]:
    """Test plusieurs URLs en parall√®le avec limitation de concurrence

    Args:
        file_path: Chemin du fichier YAML contenant les URLs
    Returns:
        Liste des r√©sultats de test
    """
    if file_path is None:
        file_path = URLS_FILE

    try:
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as file:
                data = yaml.safe_load(file)
                if isinstance(data, list):
                    data_urls = data
                else:
                    logger.error(
                        "‚ùå Format YAML incorrect pour les URLs (doit √™tre une liste)"
                    )
                    data_urls = []
        else:
            logger.error(f"‚ùå Fichier URLs non trouv√©: {file_path}")
            data_urls = []
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement des URLs: {str(e)}")
        data_urls = []

    # ‚ö° Utilisation d'un connector TCP avec r√©utilisation des connexions
    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_REQUESTS, force_close=False)

    async with aiohttp.ClientSession(connector=connector) as session:
        # üîÄ Cr√©ation des t√¢ches avec semaphore pour limiter la concurrence
        sem = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

        async def bounded_test(data):
            async with sem:
                return await check_single_url(session, data)

        # Filtrer les URLs exclues avant de lancer les tests
        filtered_data_urls = [
            data for data in data_urls if not _is_url_excluded(data.get("url", ""))
        ]
        logger.info(
            f"üîç {len(data_urls)} URLs totales, {len(data_urls) - len(filtered_data_urls)} exclues, {len(filtered_data_urls)} √† tester"
        )

        # Ex√©cution parall√®le des tests
        tasks = [bounded_test(data) for data in filtered_data_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    return [r for r in results if isinstance(r, dict)]


def _get_http_routes():
    """
    R√©cup√®re toutes les HTTPRoutes √† travers les namespaces
    Returns: list des HTTPRoutes avec leurs paths
    """
    urls_with_paths = []  # üìù Liste pour stocker les URLs et leurs paths
    try:
        # üîå Initialisation des clients K8s
        core = client.CoreV1Api()
        v1Gateway = client.CustomObjectsApi()

        # üîÑ Parcours des namespaces
        for ns in core.list_namespace().items:
            try:
                routes = v1Gateway.list_namespaced_custom_object(
                    group="gateway.networking.k8s.io",
                    version="v1beta1",
                    plural="httproutes",
                    namespace=ns.metadata.name,
                )

                # ‚ú® Traitement de chaque HTTPRoute
                for route in routes["items"]:
                    if not route.get("spec"):
                        continue

                    # üè∑Ô∏è Extraction des hostnames
                    hostnames = route["spec"].get("hostnames", [])

                    # üõ£Ô∏è Extraction des paths depuis les rules
                    paths = []
                    for rule in route["spec"].get("rules", []):
                        for match in rule.get("matches", []):
                            if path_data := match.get(
                                "path"
                            ):  # Using assignment expression
                                paths.append(
                                    {
                                        "type": path_data.get("type", "PathPrefix"),
                                        "value": path_data.get("value", "/"),
                                    }
                                )
                                logger.debug(f"Found path: {path_data}")

                    # üìù Pour chaque hostname, ajouter les paths
                    for hostname in hostnames:
                        if (
                            not paths
                        ):  # Si pas de paths d√©finis, ajouter le path par d√©faut
                            paths = [{"type": "PathPrefix", "value": "/"}]

                        urls_with_paths.append(
                            {
                                "hostname": hostname,
                                "paths": paths,
                                "namespace": ns.metadata.name,
                                "annotations": route["metadata"].get("annotations", {}),
                                "name": route["metadata"]["name"],
                            }
                        )
                        logger.debug(f"Added HTTPRoute: {hostname} with paths: {paths}")

            except Exception as e:
                logger.warning(
                    f"‚ùå Erreur lors de la lecture du namespace {ns.metadata.name}: {e}"
                )
                continue

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale: {e}")
        raise

    logger.info(f"üìä Total HTTPRoutes processed: {len(urls_with_paths)}")
    return urls_with_paths


def _is_url_excluded(url, annotations=None):
    """

    Args:
        url: URL √† v√©rifier
        annotations: Annotations de la ressource Kubernetes (optionnel)
    """
    # üéØ SOLUTION 3: V√©rification via annotation
    if (
        annotations
        and annotations.get("portal-checker.io/exclude", "").lower() == "true"
    ):
        logger.debug(f"üö´ URL exclue via annotation: {url}")
        return True

    # Normaliser l'URL en supprimant le slash final si pr√©sent
    normalized_url = url[:-1] if url.endswith("/") and len(url) > 1 else url

    # V√©rification directe avec URL normalis√©e
    for excluded in excluded_urls:
        # Normaliser aussi l'URL exclue
        normalized_excluded = (
            excluded[:-1]
            if excluded.endswith("/") and not excluded.endswith("*/")
            else excluded
        )

        # V√©rification exacte
        if normalized_url == normalized_excluded:
            logger.debug(f"üö´ URL exclue (exacte): {url}")
            return True

        # V√©rification avec wildcards
        if normalized_excluded.endswith("*") and normalized_url.startswith(
            normalized_excluded[:-1]
        ):
            logger.debug(f"üö´ URL exclue (wildcard): {url}")
            return True

        # V√©rification avec patterns complexes (*.internal/*)
        if "*" in normalized_excluded:
            import fnmatch

            if fnmatch.fnmatch(normalized_url, normalized_excluded):
                logger.debug(f"üö´ URL exclue (pattern): {url}")
                return True

    return False


# üìù Liste pour stocker toutes les URLs
def _deduplicate_urls(url_details):
    """
    Supprime les URLs dupliqu√©es en gardant la premi√®re occurrence
    Args:
        url_details: Liste des dictionnaires contenant les d√©tails des URLs
    Returns:
        Liste d√©dupliqu√©e des URLs
    """
    seen = set()
    unique_urls = []
    duplicates_count = 0
    
    for url_data in url_details:
        # Cr√©er une cl√© unique bas√©e sur URL, namespace et nom
        key = (url_data.get("url", ""), url_data.get("namespace", ""), url_data.get("name", ""))
        
        if key not in seen:
            seen.add(key)
            unique_urls.append(url_data)
        else:
            duplicates_count += 1
            logger.debug(f"üîÑ URL dupliqu√©e ignor√©e: {url_data.get('url', 'unknown')}")
    
    if duplicates_count > 0:
        logger.info(f"üîÑ {duplicates_count} URLs dupliqu√©es supprim√©es")
    
    return unique_urls


def _extract_essential_annotations(annotations):
    """
    Extrait seulement les annotations essentielles pour r√©duire l'usage m√©moire
    Args:
        annotations: Dictionnaire complet des annotations
    Returns:
        Dictionnaire avec seulement les annotations importantes
    """
    if not annotations:
        return {}
    
    # Garder seulement les annotations importantes pour le portail checker
    essential_keys = {
        'portal-checker.io/exclude',
        'cert-manager.io/cluster-issuer', 
        'ingress.kubernetes.io/ssl-redirect',
        'nginx.ingress.kubernetes.io/cors-allow-origin',
        'kubernetes.io/ingress.class'
    }
    
    # D'abord, garder toutes les annotations essentielles
    essential_annotations = {}
    other_annotations = {}
    
    for key, value in annotations.items():
        if key in essential_keys:
            essential_annotations[key] = value
        # Garder les annotations courtes (moins de 50 caract√®res)
        elif len(str(value)) < 50:
            other_annotations[key] = value
    
    # Combiner les annotations, en priorisant les essentielles
    result = essential_annotations.copy()
    
    # Ajouter les autres annotations jusqu'√† la limite de 10
    remaining_slots = 10 - len(result)
    if remaining_slots > 0:
        for key, value in list(other_annotations.items())[:remaining_slots]:
            result[key] = value
        
    return result


def _get_all_urls_with_details():
    """
    R√©cup√®re toutes les URLs avec leurs d√©tails depuis les HTTPRoutes et Ingress
    Returns: list[dict] Liste des dictionnaires contenant les d√©tails des URLs
    """
    url_details = []  # üìä Liste pour stocker les dictionnaires de d√©tails
    filtered_count = 0  # üîç Compteur pour les URLs filtr√©es

    try:
        # üåê R√©cup√©ration et traitement des HTTPRoutes
        httproute_list = _get_http_routes()
        logger.info(f"‚úÖ {len(httproute_list)} HTTPRoutes r√©cup√©r√©es")

        # Traitement des HTTPRoutes
        for route in httproute_list:
            hostname = route["hostname"]
            paths = route["paths"]
            name = route["name"]
            namespace = route["namespace"]
            annotations = route["annotations"]
            status = route.get("status", "unknown")

            if not paths:
                full_url = f"{hostname}/"
                if not _is_url_excluded(full_url, annotations):
                    url_details.append(
                        {
                            "name": name,
                            "namespace": namespace,
                            "annotations": _extract_essential_annotations(annotations),
                            "type": "HTTPRoute",
                            "url": full_url,
                            "status": status,
                            "info": "Default path",
                        }
                    )
                    logger.debug(f"Added default path for {hostname}")
                else:
                    filtered_count += 1
                    logger.debug(f"üö´ URL exclue: {full_url}")
            else:
                # üõ£Ô∏è Traitement de chaque path pour ce hostname
                for path in paths:
                    path_value = path.get("value", "/")
                    if not path_value.startswith("/"):
                        path_value = f"/{path_value}"

                    full_url = f"{hostname}{path_value}"
                    if not _is_url_excluded(full_url, annotations):
                        url_details.append(
                            {
                                "name": name,
                                "namespace": namespace,
                                "type": "HTTPRoute",
                                "annotations": _extract_essential_annotations(annotations),
                                "url": full_url,
                                "status": status,
                                "info": f"Path: {path_value}",
                            }
                        )
                        logger.debug(f"Added HTTPRoute URL: {full_url}")
                    else:
                        filtered_count += 1
                        logger.debug(f"üö´ URL exclue: {full_url}")

        logger.info(f"‚úÖ {len(url_details)} URLs HTTPRoute g√©n√©r√©es")

        # üîÑ Traitement des Ingress classiques
        try:
            v1 = client.NetworkingV1Api()
            ingress_list = v1.list_ingress_for_all_namespaces()

            for ingress in ingress_list.items:
                if not ingress.spec.rules:
                    continue

                ingress_name = ingress.metadata.name
                ingress_namespace = ingress.metadata.namespace
                ingress_status = (
                    "Active" if ingress.status.load_balancer.ingress else "Pending"
                )
                annotations = ingress.metadata.annotations or {}
                labels = ingress.metadata.labels or {}

                for rule in ingress.spec.rules:
                    if not rule.host:
                        continue

                    # Si pas de paths d√©finis, on ajoute le hostname avec /
                    if not rule.http or not rule.http.paths:
                        full_url = f"{rule.host}/"
                        if not _is_url_excluded(full_url, annotations):
                            url_details.append(
                                {
                                    "name": ingress_name,
                                    "namespace": ingress_namespace,
                                    "annotations": _extract_essential_annotations(annotations),
                                    "labels": _extract_essential_annotations(labels),  # Also optimize labels
                                    "type": "Ingress",
                                    "url": full_url,
                                    "status": ingress_status,
                                    "info": "Default path",
                                }
                            )
                            logger.debug(f"Added default Ingress path for {rule.host}")
                        else:
                            filtered_count += 1
                        continue

                    # Ajout de chaque path pour ce hostname
                    for path in rule.http.paths:
                        path_value = path.path if path.path else "/"
                        if not path_value.startswith("/"):
                            path_value = f"/{path_value}"

                        full_url = f"{rule.host}{path_value}"
                        backend_info = (
                            f"Service: {path.backend.service.name}"
                            if hasattr(path.backend, "service") and path.backend.service
                            else "No service"
                        )

                        if not _is_url_excluded(full_url, annotations):
                            url_details.append(
                                {
                                    "name": ingress_name,
                                    "namespace": ingress_namespace,
                                    "annotations": _extract_essential_annotations(annotations),
                                    "labels": _extract_essential_annotations(labels),  # Also optimize labels
                                    "type": "Ingress",
                                    "url": full_url,
                                    "status": ingress_status,
                                    "info": f"Path: {path_value}, Backend: {backend_info}",
                                }
                            )
                            logger.debug(f"Added Ingress URL: {full_url}")
                        else:
                            filtered_count += 1

            logger.info(
                f"‚úÖ {len(url_details)} URLs totales g√©n√©r√©es, {filtered_count} URLs exclues"
            )

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des Ingress: {e}")

    except Exception as e:
        logger.error(f"‚ùå Erreur inattendue: {e}")
        logger.exception(e)  # üìù Log complet de l'erreur avec stack trace

    # üîÑ D√©duplication des URLs avant retour
    url_details = _deduplicate_urls(url_details)
    logger.info(f"‚úÖ {len(url_details)} URLs uniques apr√®s d√©duplication")
    
    return url_details  # üìä Retour de la liste de dictionnaires


def _refresh_urls_if_needed():
    """
    üöÄ SOLUTION AUTO-REFRESH: Effectue un refresh automatique au d√©marrage si n√©cessaire
    """
    if not AUTO_REFRESH_ON_START:
        logger.info("üîÑ Auto-refresh d√©sactiv√© au d√©marrage")
        return

    # V√©rifier si le fichier URLs existe et n'est pas vide
    urls_exist = False
    if os.path.exists(URLS_FILE):
        try:
            with open(URLS_FILE, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f)
                if data and isinstance(data, list) and len(data) > 0:
                    urls_exist = True
                    logger.info(f"‚úÖ {len(data)} URLs d√©j√† pr√©sentes dans {URLS_FILE}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la lecture du fichier URLs: {e}")

    if not urls_exist:
        logger.info("üöÄ Cr√©ation du fichier URLs manquant - R√©cup√©ration des URLs...")
        try:
            # Initialisation de la configuration Kubernetes
            if os.getenv("FLASK_ENV") == "development":
                config.load_kube_config()
            else:
                config.load_incluster_config()

            # R√©cup√©ration et sauvegarde des URLs
            data_urls = _get_all_urls_with_details()

            # Cr√©er le r√©pertoire si n√©cessaire (m√™me en dev mode pour ce cas)
            config_dir = os.path.dirname(URLS_FILE)
            if config_dir and not os.path.exists(config_dir):
                os.makedirs(config_dir, exist_ok=True)

            # Cr√©er le fichier URLs m√™me en mode dev si il n'existe pas
            with open(URLS_FILE, "w", encoding="utf-8") as f:
                yaml.safe_dump(
                    data_urls, f, default_flow_style=False, allow_unicode=True
                )

            logger.info(
                f"‚úÖ {len(data_urls)} URLs sauvegard√©es automatiquement dans {URLS_FILE}"
            )

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation du fichier URLs: {e}")


@app.route("/refresh", methods=["GET"])
def check_all_urls():
    """Get all ingress URLs and write them to a YAML file."""
    if os.getenv("FLASK_ENV") == "development":
        config.load_kube_config()
    else:
        config.load_incluster_config()

    # R√©cup√©rer toutes les URLs
    data_urls = _get_all_urls_with_details()

    # Skip file writing in development mode to avoid read-only filesystem errors
    if os.getenv("FLASK_ENV") != "development":
        # Assurez-vous que le r√©pertoire config existe
        config_dir = os.path.dirname(URLS_FILE)
        if config_dir:
            os.makedirs(config_dir, exist_ok=True)

        with open(URLS_FILE, "w", encoding="utf-8") as f:
            yaml.safe_dump(data_urls, f, default_flow_style=False, allow_unicode=True)

    if os.getenv("FLASK_ENV") != "development":
        logger.info(f"‚úÖ {len(data_urls)} URLs sauvegard√©es dans {URLS_FILE}")
    else:
        logger.info(f"‚úÖ {len(data_urls)} URLs g√©n√©r√©es (dev mode - skip file write)")

    origin_url = request.referrer
    return redirect(origin_url) if origin_url else redirect("/")


@app.route("/urls", methods=["GET"])
def get_urls():
    """Get all URLs from the YAML file."""
    if os.path.exists(URLS_FILE):
        with open(URLS_FILE, "r", encoding="utf-8") as f:
            data_urls = yaml.safe_load(f)
        # drop all data except url and name
        data_urls = [{"url": url["url"], "name": url["name"]} for url in data_urls]
        return jsonify(data_urls)
    else:
        return jsonify([])


@app.route("/static/favicon.ico")
def favicon():
    return send_from_directory(
        os.path.join(app.root_path, "static"), "favicon.ico", mimetype="image/ico"
    )


@app.route("/static/image.png")
def logo():
    return send_from_directory(
        os.path.join(app.root_path, "static"), "image.png", mimetype="image/png"
    )


@app.route("/health")
def health():
    """Endpoint de sant√© pour v√©rifier que l'application est en ligne"""
    logger.debug("ok")
    return {"status": "ok"}, 200


@app.route("/memory")
def memory_status():
    """Memory usage endpoint for monitoring"""
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return jsonify({
            "memory_rss_mb": round(memory_info.rss / 1024 / 1024, 2),
            "memory_vms_mb": round(memory_info.vms / 1024 / 1024, 2),
            "memory_percent": round(process.memory_percent(), 2),
            "status": "ok"
        })
    except ImportError:
        return jsonify({"error": "psutil not available - add it to dependencies"}), 500


@app.route("/")
async def index():
    """Point d'entr√©e principal avec gestion asynchrone"""

    results = await check_urls_async()
    logger.info(f"‚úÖ {len(results)} URLs test√©es")

    return render_template("index.html", results=results, version=get_app_version())


# üöÄ Initialisation au d√©marrage
_refresh_urls_if_needed()


if __name__ == "__main__":
    if os.getenv("FLASK_ENV") == "development":
        # Mode d√©veloppement avec auto-reload
        port = int(os.getenv("PORT", 5001))
        app.run(debug=True, host="0.0.0.0", port=port)
    else:
        # Mode production avec Hypercorn
        from asgiref.wsgi import WsgiToAsgi
        from hypercorn.asyncio import serve
        from hypercorn.config import Config
        from werkzeug.middleware.proxy_fix import ProxyFix

        logger.info("Application started with hypercorn")
        ic.disable()
        app.wsgi_app = ProxyFix(app.wsgi_app, x_for=1, x_proto=1, x_host=1, x_prefix=1)
        asgi_app = WsgiToAsgi(app)
        asyncio.run(serve(asgi_app, Config()))
